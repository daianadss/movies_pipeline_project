# Pipeline ETL de Filmes

Projeto de Engenharia de Dados focado na construção de um pipeline ETL (Extract, Transform, Load) para dados de filmes. O objetivo é praticar as etapas de extração, limpeza, transformação e carga de dados, resultando em um conjunto de dados pronto para análise e, algumas análises iniciais.

---

## Dados

Os datasets brutos utilizados neste projeto são:
- `movies_metadata.csv`: Contém metadados de aproximadamente 45.000 filmes.
- `ratings.csv`: Contém 26 milhões de avaliações de filmes.

Ambos os datasets foram obtidos do [The Movies Dataset no Kaggle](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset/).

**Para replicar o projeto, por favor, baixe estes arquivos diretamente do Kaggle e coloque-os na pasta `data/raw/` do seu projeto clonado.**

A pasta `data/processed/` (onde o `movies_metadata_cleaned.csv` é salvo após a fase de Transformação) **não é versionada no Git** para manter o repositório leve e focado no código.

---

## Estrutura do Projeto

- `data/`: Contém os datasets utilizados. **Esta pasta é ignorada pelo Git.**
    - `data/raw/`: Dados brutos originais. 
    - `data/processed/`: Dados após as etapas de limpeza e transformação. 
- `scripts/`: Contém os scripts Python que realizam as operações de ETL e análise exploratória.
    - `01_inspecao_movies_metadata.py`: Script principal que realiza a extração, limpeza e transformação dos metadados dos filmes, além de integrar dados de avaliações.
    - `02_load_data_to_sqlite.py` (A ser implementado): Script responsável por carregar os dados transformados em um banco de dados SQLite.
- `.gitignore`: Configura quais arquivos e pastas o Git deve ignorar ao versionar o projeto.

---

## Fases do Pipeline ETL

### Fase de Extração (E)

Os dados são extraídos diretamente dos arquivos CSV brutos (`movies_metadata.csv` e `ratings.csv`) localizados na pasta `data/raw/`.

### Fase de Transformação (T)

Esta fase, detalhada no script `01_inspecao_movies_metadata.py`, envolveu a limpeza e o enriquecimento dos datasets:

-   **Tratamento de Dados Aninhados:** Extração e normalização de informações de dicionários e listas aninhadas em colunas como `genres`, `production_companies`, `spoken_languages`, etc.
-   **Limpeza e Conversão de Tipos:**
    -   `release_date`: Convertida para o tipo `datetime`, e extraído o `release_year`.
    -   `budget` e `revenue`: Convertidas para o tipo numérico (`float`), com tratamento de valores não numéricos e preenchimento de nulos com `0`.
    -   `id`: Garantido como numérico (`int`) e sem valores nulos.
    -   `vote_average` e `vote_count`: Convertidas para numérico e nulos preenchidos com `0`.
-   **Tratamento de Valores Ausentes:** Aplicação de estratégias como remoção de linhas (para `title`, `id`, `release_date`) e preenchimento de nulos (para colunas numéricas de votos, orçamento e receita) para garantir a integridade dos dados.
-   **Agregação e Enriquecimento:**
    -   O dataset `ratings.csv` foi agregado para calcular a `average_rating` (média de avaliações) e `ratings_count` (contagem de avaliações) por filme.
    -   Essas métricas foram então mescladas (`left join`) com o `df_metadata_clean` para criar um `df_final` enriquecido, contendo os metadados dos filmes e suas informações de avaliação.
-   **Persistência:** O `df_final` foi salvo em formato CSV (`movies_metadata_cleaned.csv`) na pasta `data/processed/`, marcando o fim da fase de Transformação.

### Fase de Carga (L) - Próximos Passos

A próxima etapa é carregar o dataset processado (`movies_metadata_cleaned.csv`) em um banco de dados relacional (SQLite) para análises e consultas posteriores. Esta fase será implementada no script `02_load_data_to_sqlite.py`.

---

## Como Executar

1.  **Pré-requisitos:** Certifique-se de ter Python (versão 3.x) e as bibliotecas necessárias instaladas (e.g., `pandas`, `numpy`, `sqlite3`). Recomenda-se usar um ambiente virtual.
2.  **Clone o Repositório:**
    ```bash
    git clone https://github.com/daianadss/movies_pipeline_project.git
    cd movies_pipeline_project
    ```
3.  **Baixe os Dados Brutos:** Acesse o link do Kaggle fornecido na seção "Dados" acima e baixe os arquivos `movies_metadata.csv` e `ratings.csv`.
4.  **Organize os Dados Brutos:** Crie uma pasta `data/raw/` na raiz do seu projeto e coloque os arquivos CSV baixados dentro dela.
5.  **Execute a Fase de Transformação:**
    ```bash
    python notebooks/01_inspecao_movies_metadata.py
    ```
    Este script irá limpar, transformar e salvar o dataset `movies_metadata_cleaned.csv` na pasta `data/processed/`.
6.  **Execute a Fase de Carga (Futuro):** Após a implementação, o script `02_load_data_to_sqlite.py` será executado para carregar os dados processados no SQLite.

---
